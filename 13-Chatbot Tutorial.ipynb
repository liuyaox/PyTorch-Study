{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Tutorial\n",
    "\n",
    "## 1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = 'cornell_movie_dialogs_corpus'\n",
    "corpus = os.path.join('data', corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'rb') as fr:\n",
    "        lines = fr.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "printLines(os.path.join(corpus, 'movie_lines.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create formatted data file\n",
    "\n",
    "Formatted data file: each line contains ***a tab-separated query sentence*** and ***a response sentence*** pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits each line into a dictionary: \n",
    "# lines是字典，每个元素line：key=lineID, value=lineObj={lineID:xxx, characterID:xxx, movieID:xxx, character:xxx, text:xxx}\n",
    "def loadLines(file, cols):\n",
    "    lines = {}\n",
    "    with open(file, 'r', encoding='iso-8859-1') as fr:\n",
    "        for line in fr:\n",
    "            values = line.split(' +++$+++ ')\n",
    "            lineObj = {col: values[i] for i, col in enumerate(cols)}\n",
    "            lines[lineObj['lineID']] = lineObj\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups cols of lines from `loadLines` into conversations based on movie_conversations.txt\n",
    "# conversations是列表，每个元素convObj: {col1:xxx, col2:xxx, ..., lines: [lineObj1, lineObj2, ..., lineObjm]}\n",
    "def loadConversations(file, lines, cols):\n",
    "    conversations = []\n",
    "    with open(file, 'r', encoding='iso-8859-1') as fr:\n",
    "        for line in fr:\n",
    "            values = line.split(' +++$+++ ')\n",
    "            convObj = {col: values[i] for i, col in enumerate(cols)}\n",
    "            convObj['lines'] = [lines[lineId] for lineId in eval(convObj['utteranceIDs'])]\n",
    "            conversations.append(convObj)\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 刘尧：训练数据是每个conversation中所有sentence生成的sentece对: <前一句话, 后一句话\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts pairs of sentences from conversations\n",
    "# qa_pair是列表，每个元素是sentence pair: [conv1_text1,conv1_text2], [1_2,1_3], [1_3,1_4], [1_4,1_5], ..., [2_1,2_2], [2_2,2_3], ...\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pair = []\n",
    "    for conv in conversations:\n",
    "        for i in range(len(conv['lines']) - 1):  # Ignore the last line (no answer for it)\n",
    "            inputLine = conv['lines'][i]['text'].strip()\n",
    "            targetLine = conv['lines'][i + 1]['text'].strip()\n",
    "            if inputLine and targetLine:\n",
    "                qa_pair.append([inputLine, targetLine])\n",
    "    return qa_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join(corpus, 'formatted_movie_lines.txt')\n",
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter, 'unicode_escape'))  # 有时间好好研究这句话！！！\n",
    "\n",
    "lines = {}\n",
    "conversations = []\n",
    "MOVIE_LINES_COLS = ['lineID', 'characterID', 'movieID', 'character', 'text']\n",
    "MOVIE_CONVERSATIONS_COLS = ['character1ID', 'character2ID', 'movieID', 'utteranceIDs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus and loading conversations...\n"
     ]
    }
   ],
   "source": [
    "print('\\nProcessing corpus and loading conversations...')\n",
    "lines = loadLines(os.path.join(corpus, 'movie_lines.txt'), MOVIE_LINES_COLS)\n",
    "conversations = loadConversations(os.path.join(corpus, 'movie_conversations.txt'), lines, MOVIE_CONVERSATIONS_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing newly formatted file ...\n",
      "\n",
      "Sample lines from file: \n",
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n"
     ]
    }
   ],
   "source": [
    "print('\\nWriting newly formatted file ...')\n",
    "with open(datafile, 'w', encoding='utf8') as fw:\n",
    "    writer = csv.writer(fw, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extractSentencePairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "print('\\nSample lines from file: ')\n",
    "printLines(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and trim data\n",
    "\n",
    "> 刘尧：把vocabulary及其附属或衍生变量以及相关method封装抽象成Class！这样既保护数据又方便使用！好好好！！！\n",
    "\n",
    "Note that we are dealing with sequences of **words**, we should create a **vocabulary**: mapping each unique word that we encounter in our dataset to an index value.\n",
    "\n",
    "For this we define a ***Vocabulary*** class, which has 5 attributes and 3 methods:\n",
    "\n",
    "- 5 attributes\n",
    "\n",
    "    - **word2index**: A mapping from each word to index\n",
    "\n",
    "    - **index2word**: A reverse mapping from index to each word\n",
    "\n",
    "    - **word2count**: A mapping from each word to its count\n",
    "\n",
    "    - num_words: A total word count\n",
    "    \n",
    "    - trimmed: If infrequently seen words are trimmed\n",
    "\n",
    "- 3 methods\n",
    "\n",
    "    - **addWord**: Adding a word to the vacabulary\n",
    "\n",
    "    - addSentence: Adding all words in a sentence\n",
    "\n",
    "    - trim: Trimming infrequently seen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1  # Start of sentence\n",
    "EOS_TOKEN = 2  # End of sentence \n",
    "\n",
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}  # 不默认包含PAD,SOS,EOS这仨\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_TOKEN: 'PAD', SOS_TOKEN: 'SOS', EOS_TOKEN: 'EOS'}\n",
    "        self.num_words = 3  # SOS, EOS, PAD\n",
    "        \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words    # 添加的word，其index依次往后排\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        \n",
    "        keep_words = [k for k, v in self.word2count.items() if v >= min_count]\n",
    "        print(f'keep_words {len(keep_words)} / {len(self.word2index)} = {len(keep_words) / len(self.word2index): .4f}')\n",
    "        \n",
    "        # Reinitializa dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_TOKEN: 'PAD', SOS_TOKEN: 'SOS', EOS_TOKEN: 'EOS'}\n",
    "        self.num_words = 3\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "            \n",
    "        self.trimmed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data preprocessing:\n",
    "\n",
    "- **unicodeToAscii**: Convert the Unicode strings to ASCII\n",
    "\n",
    "- **normalizeString**: Convert all letters to lowercase and trim all non-letter characters except for basic punctuation\n",
    "\n",
    "- **filterPairs**: Filter sentences with length greater than the *MAX_LENGTH* threshold\n",
    "\n",
    "> 刘尧：这些常规的预处理，最好封装成一个个function，以方便使用！可以放在**Coding通用工具脚本**里！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    \"\"\"Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    \"\"\"Lowercase, trim, and remove non-letter characters\"\"\"\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r'([.!?])', r' \\1', s)      # 把.!?三个标点符号替换为？\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)  # 把字母和.!?之外的character替换为空格\n",
    "    s = re.sub(r'\\s+', r' ', s).strip()    # 把替换为空格\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readVocs(datafile, corpus_name):\n",
    "    \"\"\"Read <query, response> pairs and return a Vocabulary object\"\"\"\n",
    "    lines = open(datafile, encoding='utf8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in line.split('\\t')] for line in lines]\n",
    "    voc = Vocabulary(corpus_name)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(pair):\n",
    "    \"\"\"Return True iff both sentences in pair are under the MAX_LENGTH threshold\"\"\"\n",
    "    return len(pair[0].split(' ')) < MAX_LENGTH and len(pair[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    \"\"\"Filter pairs using filterPair function\"\"\"\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):  # corpus, save_dir 在哪里使用的！？！\n",
    "    \"\"\"Using the functions above, return a populated Vocabulary object and pairs list\"\"\"\n",
    "    print('Start preparing training data ...')\n",
    "    vocabulary, pairs = readVocs(datafile, corpus_name)\n",
    "    print('Read {!s} sentence pairs'.format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print('Trimmed to {!s} sentence pairs'.format(len(pairs)))\n",
    "    print('Counting words ...')\n",
    "    for pair in pairs:\n",
    "        vocabulary.addSentence(pair[0])\n",
    "        vocabulary.addSentence(pair[1])\n",
    "    print('Counted words: ', vocabulary.num_words)\n",
    "    return vocabulary, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Read 221282 sentence pairs\n",
      "Trimmed to 64271 sentence pairs\n",
      "Counting words ...\n",
      "Counted words:  18008\n"
     ]
    }
   ],
   "source": [
    "# Load/Assemble vocabulary and pairs\n",
    "save_dir = os.path.join('data', 'save')\n",
    "vocabulary, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tactic that is beneficial to achieving faster convergence during training is **trimming rarely used words out of our vocabulary**. \n",
    "\n",
    "Decreasing the feature space will also soften the difficulty of the function that the model must learn to approximate.\n",
    "\n",
    "We will do this as a two-stage process:\n",
    "    \n",
    "- Trim words used under *MIN_COUNT* threshold using the *Vocabulary.trim* function\n",
    "\n",
    "- Filter out pairs with trimmed words\n",
    "\n",
    "> 刘尧：事先从Vocabulary中定义并删除不常见的word，即**OOV的word**，随后从训练数据中删除这些OOV的word！ \n",
    "\n",
    "> 刘尧：疑问：模型应用时遇到OOV的word咋办？？？跟训练一样，应用前也先使用trimRareWords来处理一下！？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 3\n",
    "def trimRareWords(vocabulary, pairs, MIN_COUNT):\n",
    "    \"\"\"基于MIN_COUNT，删除vocabulary中不常见的word，并从训练/应用数据中删除带有不常见word的pairs\"\"\"\n",
    "    vocabulary.trim(MIN_COUNT)\n",
    "    \n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input, keep_output = True, True\n",
    "        \n",
    "        # 判断pairs中2个句子中是否存在OOV的word，一旦存在，则删除当前pairs\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in vocabulary.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in vocabulary.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    print(f'Trimmed from {len(pairs)} pairs to {len(keep_pairs)}, {len(keep_pairs) / len(pairs): .4f} of total')\n",
    "    return keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7823 / 18005 =  0.4345\n",
      "Trimmed from 64271 pairs to 53165,  0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "pairs = trimRareWords(vocabulary, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Models\n",
    "\n",
    "### Seq2Seq Model\n",
    "\n",
    "\n",
    "\n",
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Training Procedure\n",
    "\n",
    "### Masked loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Evaluation\n",
    "\n",
    "### Greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate my text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Model\n",
    "\n",
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
